<img src="https://i.imgur.com/lQJojh4.png"/>

# üõí Super Mercado üõçÔ∏è


Este projeto permitir√° que a rede de supermercados compreenda melhor as intera√ß√µes entre clientes e funcion√°rios, identificando oportunidades de aprimoramento no atendimento. Aumentando a satisfa√ß√£o do cliente e melhorando a efici√™ncia, a empresa poder√° criar uma experi√™ncia mais positiva para seus clientes, promovendo a fideliza√ß√£o e o sucesso a longo prazo.A analise pode ser conferida [aqui](https://github.com/Jandersolutions/super_mercado/blob/main/notebooks/projeto.ipynb)  üåüü§ù

## Inicializando o projeto üöÄ


- Clone o projeto: ```git clone https://github.com/Jandersolutions/super_mercado.git```
- Crie uma m√°quina virtual: ```python -m venv myvenv```
- Ative a m√°quina virtual: ```source myvenv/bin/activate```
- Instale as requisi√ß√µes: ```pip install -r requirements.txt```
- Abra o JupyterLab: ```jupyter-lab``` üìäüî¨
- Docker:
  ```docker build -t janderscience .```
  ```docker run -p 8888:8888 janderscience```

## Materiais e links üìï


------------
| Material | Link |
|---------|------|
| Notebook | https://github.com/Jandersolutions/super_mercado/blob/main/notebooks/projeto.ipynb |
| Docker | https://docs.docker.com/ |
| Report do projeto | https://github.com/Jandersolutions/super_mercado/blob/main/reports/relatorio.pdf |
| Gerador de Dataset¬π | https://github.com/Jandersolutions/super_mercado/blob/main/src/data/geraDatasetVendas.py |
| Dataset | https://github.com/Jandersolutions/super_mercado/blob/main/data/raw/venda_07-2023.csv |
| Cookiecutter Data Science | https://drivendata.github.io/cookiecutter-data-science/ |
| CRISP-DM | https://pt.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining |
| Faker | https://faker.readthedocs.io/en/master/ |
| Documenta√ß√£o do projeto |https://github.com/Jandersolutions/super_mercado/blob/main/references/descric%C3%A3o%20do%20projeto.pdf |
| Dicion√°rio de dados | https://github.com/Jandersolutions/super_mercado/blob/main/references/dicionario%20dos%20dados.pdf |

1 artigo que publiquei sobre o gerador de dataset -> [artigo](https://web.dio.me/articles/gerador-de-dataset-sintetico-gere-seus-arquivos-csvs?back=%2Farticles&page=1&order=oldest)
## Explicando o CRISP-DM:

O CRISP-DM, que significa "Cross Industry Standard Process for Data Mining", √© um modelo de processo amplamente utilizado para a minera√ß√£o de dados e an√°lise de dados. Ele fornece uma estrutura e orienta√ß√£o para as etapas envolvidas no desenvolvimento de projetos de minera√ß√£o de dados e an√°lise de dados. O CRISP-DM √© composto por v√°rias fases interconectadas e iterativas, permitindo uma abordagem sistem√°tica para resolver problemas de minera√ß√£o de dados e alcan√ßar insights a partir de grandes conjuntos de dados.
<img src="https://i.imgur.com/1MjYpWh.png"/>

As fases do modelo CRISP-DM s√£o as seguintes:

-**Entendimento do Neg√≥cio (Business Understanding)**: Nesta fase, os objetivos do projeto s√£o definidos, identificando-se as metas de neg√≥cios que a an√°lise de dados deve alcan√ßar. √â crucial compreender o contexto do problema e como a an√°lise de dados pode agregar valor ao neg√≥cio.

-**Entendimento dos Dados (Data Understanding)**: Aqui, a equipe analisa e explora os dados dispon√≠veis para entender sua estrutura, qualidade e natureza. Isso envolve coletar informa√ß√µes sobre os dados, identificar padr√µes iniciais e compreender o que os dados podem revelar.

-**Prepara√ß√£o dos Dados (Data Preparation)**: Nesta fase, os dados s√£o selecionados, limpos, transformados e formatados para serem adequados para a an√°lise. √â um est√°gio cr√≠tico para garantir que os dados estejam prontos para serem usados nos modelos de minera√ß√£o.

-**Modelagem (Modeling)**: Aqui, diferentes modelos de minera√ß√£o de dados s√£o criados e avaliados. Esses modelos podem incluir algoritmos de aprendizado de m√°quina, t√©cnicas estat√≠sticas e outras abordagens para gerar previs√µes, classifica√ß√µes ou outras an√°lises.

-**Avalia√ß√£o (Evaluation)**: Nesta fase, os modelos criados s√£o avaliados e comparados para determinar qual deles √© o mais adequado para atender aos objetivos do projeto. A avalia√ß√£o √© feita com base em m√©tricas de desempenho relevantes para o problema em quest√£o.

-**Implanta√ß√£o (Deployment)**: Uma vez que um modelo satisfat√≥rio √© selecionado, ele √© implantado em um ambiente operacional ou integrado a um fluxo de trabalho existente. Isso pode envolver a cria√ß√£o de sistemas automatizados para fazer previs√µes ou recomenda√ß√µes com base no modelo.

-**Monitoramento (Monitoring)**: Ap√≥s a implanta√ß√£o, √© importante monitorar o desempenho cont√≠nuo do modelo e seus resultados. Se necess√°rio, ajustes e otimiza√ß√µes podem ser feitos para garantir que o modelo continue fornecendo insights precisos e relevantes ao longo do tempo.



## Como o projeto esta organizado  üìö


    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile com comandos como make data ou make train
    ‚îú‚îÄ‚îÄ README.md          <- O README de n√≠vel superior para desenvolvedores que utilizam este projeto.o arquvo que voc√™ esta lendo
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Dados de fontes de terceiros.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Dados intermedi√°rios que foram transformados.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- Conjuntos de dados finais e can√¥nicos para modelagem.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- O dado original e imut√°vel.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- Um projeto Sphinx padr√£o; veja sphinx-doc.org para detalhes.
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Modelos treinados e serializados, previs√µes de modelos ou resumos de modelos.
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Notebooks Jupyter. A conven√ß√£o de nomenclatura √© um n√∫mero (para ordena√ß√£o),
    ‚îÇ                         
    ‚îÇ                         
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Dicion√°rios de dados, manuais e todos os outros materiais explicativos.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- An√°lises geradas como HTML, PDF, LaTeX, etc
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <-  Gr√°ficos e figuras geradas para uso em relat√≥rios
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- O arquivo de requisitos para reproduzir o ambiente de an√°lise
    ‚îÇ                      
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- Torna o projeto instal√°vel via pip (pip install -e .), para que src possa ser importado.
    ‚îú‚îÄ‚îÄ src                <- C√≥digo-fonte para uso neste projeto
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts para baixar ou gerar dados.
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ make_dataset.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts para transformar dados brutos em recursos para modelagem.
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ build_features.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts para treinar modelos e usar modelos treinados para fazer previs√µes
    ‚îÇ   ‚îÇ   ‚îÇ                
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict_model.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_model.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts para criar visualiza√ß√µes explorat√≥rias e orientadas por resultados.
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ visualize.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- Arquivo tox com configura√ß√µes para executar o tox; veja tox.readthedocs.io


--------

<p><small>Este projeto foi baseado nas melhores praticas <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
